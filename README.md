# README

–í –¥–∞–Ω–Ω–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ:

1. –í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª –≤—ã–±—Ä–∞–Ω dataset **2015 Flight Delays and Cancellations** (–æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –¥–æ 100.000 —Å—Ç—Ä–æ–∫). –í –Ω–µ–º 31 –ø—Ä–∏–∑–Ω–∞–∫, –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —è–≤–ª—è—é—Ç—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä DAY_OF_WEEK).
2. –ë—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã Docker —Ñ–∞–π–ª—ã —Å namenode, 1 datanode/3 datanodes –∏ Spark.
3. –°–æ–∑–¥–∞–Ω–æ 2 Spark Application: –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ—Ä—è–µ—Ç—Å—è –≤—Ä–µ–º—è –∏ RAM, –≤ –¥—Ä—É–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ repartition –∏ persist)

–ß—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É –Ω—É–∂–Ω–æ:

1. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä:
    
    ```bash
        docker-compose -f docker-compose-1-datanode.yml up -d
    ```
    
    –∏–ª–∏ 
    
    ```bash
        docker-compose -f docker-compose-3-datanodes.yml up -d
    ```
    
2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ HDFS:
    
    ```bash
        docker exec namenode bash /scripts/upload-data.sh 1
    ```
    
    –∏–ª–∏
    
    ```bash
        docker exec namenode bash /scripts/upload-data.sh 3
    ```
    
3. –ó–∞–ø—É—Å—Ç–∏—Ç—å Spark Application
    
    ```bash
        docker exec spark bash /scripts/run-spark.sh 1 base
    ```
    
    –∏–ª–∏
    
    ```bash
        docker exec spark bash /scripts/run-spark.sh 1 optimized
    ```
    
    –∏–ª–∏
    
    ```bash
        docker exec spark bash /scripts/run-spark.sh 3 base
    ```
    
    –∏–ª–∏
    
    ```bash
        docker exec spark bash /scripts/run-spark.sh 3 optimized
    ```
    

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä:

```bash
docker-compose -f —Ñ–∞–π–ª_compose.yml down -v
```

–ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –ø–æ—è–≤–∏—Ç—Å—è –ª–æ–≥ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ results

–ù–∞ –º–æ–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –∫–æ–º–∞–Ω–¥—ã 1 –∏ 2 –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —É—Å–ø–µ—à–Ω–æ. –û–¥–Ω–∞–∫–æ 3 —à–∞–≥ –≤—ã–¥–∞–µ—Ç –æ—à–∏–±–∫—É, —Å–≤—è–∑–∞–Ω–Ω—É—é —Å —Ç–µ–º, —á—Ç–æ Spark –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (—è–¥–µ—Ä/–ø–æ—Ç–æ–∫–æ–≤) –¥–ª—è —ç–∫–∑–µ–∫—å—é—Ç–æ—Ä–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS: WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources. –ù–∞ —Å–ª–µ–¥—É—é—â–µ–º —Å–∫—Ä–∏–Ω–µ –≤–∏–¥–Ω–æ, —á—Ç–æ –¥–ª—è —ç–∫–∑–µ–∫—å—é—Ç–æ—Ä–∞ –Ω–µ –±—ã–ª–æ –≤—ã–¥–µ–ª–µ–Ω–æ —è–¥–µ—Ä –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:

![image](https://github.com/user-attachments/assets/c11fa877-5f54-4cda-9330-6b23450100b6)


–Ø –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ —Ä–∞–∑–Ω—ã–µ –æ–±—Ä–∞–∑—ã –¥–ª—è Spark: bde2020 –∏ apache/spark, –Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ –ø—Ä–æ–ø–∞–¥–∞–ª–∞.

–í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, —è –Ω–µ —Å–º–æ–≥–ª–∞ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã üò¢
